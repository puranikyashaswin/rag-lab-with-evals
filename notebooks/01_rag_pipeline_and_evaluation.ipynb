{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG pipeline and RAGAs evaluation\n",
    "This notebook builds a minimal RAG system over the abstract of 'Attention Is All You Need' and evaluates it using RAGAs on faithfulness, answer relevancy, context precision, and context recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: imports and environment check\n",
    "import os\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import faithfulness, answer_relevancy, context_precision, context_recall\n",
    "from ragas.llms import LangchainLLM\n",
    "from ragas.embeddings import LangchainEmbeddings\n",
    "\n",
    "from src.pipeline import (\n",
    "    load_and_chunk_document,\n",
    "    get_embeddings,\n",
    "    create_faiss_vectorstore,\n",
    "    get_default_llm,\n",
    "    make_rag_chain,\n",
    "    get_contexts_for_questions,\n",
    ")\n",
    "\n",
    "DATA_PATH = 'data/raw/attention_is_all_you_need.txt'\n",
    "assert os.path.exists(DATA_PATH), f'Expected data at {DATA_PATH} not found'\n",
    "\n",
    "# Verify GROQ_API_KEY is available\n",
    "if not os.getenv('GROQ_API_KEY'):\n",
    "    raise RuntimeError('Please set GROQ_API_KEY in your environment before running this notebook.')\n",
    "\n",
    "print('Environment OK. Proceeding...')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build RAG chain\n",
    "We load + chunk the document, create a FAISS vector store with bge-small-en-v1.5 embeddings, and create a simple RAG chain using Groq Llama3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load & chunk\n",
    "docs = load_and_chunk_document(DATA_PATH, chunk_size=500, chunk_overlap=60)\n",
    "print(f'Loaded and split into {len(docs)} chunks')\n",
    "\n",
    "# Embeddings and vector store\n",
    "embeddings = get_embeddings()\n",
    "vectorstore, retriever = create_faiss_vectorstore(docs, embeddings)\n",
    "\n",
    "# LLM and chain\n",
    "llm = get_default_llm()\n",
    "rag_chain = make_rag_chain(retriever, llm)\n",
    "print('RAG chain ready.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a small evaluation set\n",
    "We define questions and write reference answers (ground truth) based on the abstract. The contexts column is populated by retrieving top-k chunks for each question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    'What new architecture does the paper propose?',\n",
    "    'Which types of neural networks does the Transformer dispense with?',\n",
    "    'What BLEU score does the model achieve on WMT 2014 English-to-German?',\n",
    "    'What BLEU score does it achieve on WMT 2014 English-to-French?',\n",
    "    'How long did training take and on what hardware?',\n",
    "    'Does the Transformer generalize to other tasks? If so, which and with what score?',\n",
    "    'Why are the proposed models faster to train?',\n",
    "]\n",
    "\n",
    "ground_truth = [\n",
    "    'The Transformer, a simple architecture based solely on attention mechanisms.',\n",
    "    'Recurrent and convolutional neural networks.',\n",
    "    '28.4 BLEU on the WMT 2014 English-to-German task.',\n",
    "    '41.8 BLEU on the WMT 2014 English-to-French task.',\n",
    "    'About 3.5 days of training on eight GPUs.',\n",
    "    'Yes. It generalizes to English constituency parsing, achieving 92.7 F1 with pretraining.',\n",
    "    'They are more parallelizable, leading to significantly less training time.',\n",
    "]\n",
    "\n",
    "contexts = get_contexts_for_questions(vectorstore, questions, k=4)\n",
    "print(f'Prepared contexts for {len(questions)} questions.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate answers with the RAG chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [{'question': q} for q in questions]\n",
    "answers = rag_chain.batch(inputs)\n",
    "for q, a in list(zip(questions, answers))[:2]:\n",
    "    print('Q:', q)\n",
    "    print('A:', a)\n",
    "    print('-'*60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate with RAGAs\n",
    "We evaluate on four metrics: faithfulness, answer_relevancy, context_precision, context_recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'question': questions,\n",
    "    'answer': answers,\n",
    "    'contexts': contexts,\n",
    "    'ground_truth': ground_truth,\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# Configure RAGAs to use the same LLM and embeddings\n",
    "ragas_llm = LangchainLLM(llm=llm)\n",
    "ragas_embeddings = LangchainEmbeddings(embeddings=embeddings)\n",
    "\n",
    "metrics = [faithfulness, answer_relevancy, context_precision, context_recall]\n",
    "result = evaluate(dataset=dataset, metrics=metrics, llm=ragas_llm, embeddings=ragas_embeddings)\n",
    "\n",
    "# Aggregate scores\n",
    "scores_df = result.to_pandas()\n",
    "metric_names = [m.name for m in metrics]\n",
    "mean_scores = scores_df[metric_names].mean().to_dict()\n",
    "print('RAGAs metrics (mean over eval set):')\n",
    "for k, v in mean_scores.items():\n",
    "    print(f'- {k}: {v:.3f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
